{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The Full Dataset\n",
    "## Part 1: Data Extraction, GMM Fitting, and Visualization\n",
    "\n",
    "This part focuses on loading the event log, converting it into a Pandas DataFrame, and extracting the \"hour of day\" from timestamps. A Gaussian Mixture Model (GMM) is then fitted to this extracted feature to identify patterns in the time-of-day data. The best GMM is selected based on the Bayesian Information Criterion (BIC), and its fit along with individual components are visualized.\n",
    "\n",
    "## Implementation Steps:\n",
    "\n",
    "1. **Load Data**: Import the BPI 2017 event log and convert it to a DataFrame.\n",
    "2. **Extract Features**: Extract the \"hour of day\" from the timestamp.\n",
    "3. **Fit GMM**: Fit a GMM for a range of components and select the model with the lowest BIC.\n",
    "4. **Visualization**: Plot the density estimation and individual components of the best GMM.\n",
    "5. **BIC Score**: Print the BIC score of the optimal GMM to evaluate its fit.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3a327a213ad266d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pm4py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load BPI Challenge 2017 data\n",
    "BPI2017 = \"./BPI Challenge 2017 - Offer log.xes\"\n",
    "event_log = pm4py.read_xes(BPI2017)\n",
    "data = pm4py.convert_to_dataframe(event_log)\n",
    "\n",
    "# Extract hour of day from timestamps\n",
    "data['time_of_day'] = data['time:timestamp'].dt.hour + data['time:timestamp'].dt.minute / 60\n",
    "master_data = data\n",
    "# Prepare data for GMM fitting\n",
    "X = data['time_of_day'].values.reshape(-1, 1)\n",
    "\n",
    "# Fit GMM and find the best model based on BIC\n",
    "lowest_bic = np.infty\n",
    "best_gmm = None\n",
    "bic_list = []\n",
    "n_components_range = range(1, 11)\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=0)\n",
    "    gmm.fit(X)\n",
    "    bic = gmm.bic(X)\n",
    "    bic_list.append(bic)\n",
    "    if bic < lowest_bic:\n",
    "        lowest_bic, best_gmm = bic, gmm\n",
    "    \n",
    "# Show the best GMM\n",
    "print(f\"The best GMM has {best_gmm.n_components} components with a BIC score of {lowest_bic:.2f}.\")\n",
    "# Number of bins for the histogram\n",
    "bins = 24\n",
    "\n",
    "# Histogram data\n",
    "hist_data, bin_edges = np.histogram(X.flatten(), bins=bins, density=True)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Plot the histogram of the hour of day and the density estimated by the best GMM\n",
    "plt.figure()\n",
    "plt.hist(X.flatten(), bins=24, density=True, alpha=0.6, label='Data Histogram')\n",
    "x = np.linspace(X.min(), X.max(), 10000).reshape(-1, 1)\n",
    "logprob = best_gmm.score_samples(x)\n",
    "\n",
    "\n",
    "# Plot each individual GMM component\n",
    "for i in range(best_gmm.n_components):\n",
    "    weight = best_gmm.weights_[i]\n",
    "    mean = best_gmm.means_[i, 0]\n",
    "    covariance = best_gmm.covariances_[i, 0, 0]\n",
    "    individual_pdf = weight * norm.pdf(x, mean, np.sqrt(covariance))\n",
    "    plt.plot(x, individual_pdf, '--', label=f'Component {i+1}')\n",
    "\n",
    "# GMM parameters\n",
    "gmm_means = best_gmm.means_.flatten()\n",
    "gmm_variances = best_gmm.covariances_.flatten()\n",
    "gmm_weights = best_gmm.weights_.flatten()\n",
    "\n",
    "# Now, print or store the values you need\n",
    "print(\"Histogram Bin Centers:\", bin_centers)\n",
    "print(\"Histogram Bin Heights:\", hist_data)\n",
    "print(\"Bin Edges:\", bin_edges)\n",
    "print(\"GMM Means:\", gmm_means)\n",
    "print(\"GMM Variances:\", gmm_variances)\n",
    "print(\"GMM Weights:\", gmm_weights)\n",
    "\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf, '-k', label='GMM Fit')\n",
    "plt.title('Hour of Day and GMM Density Estimation')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c527ce9da0e87c0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Segmented Dataset\n",
    "\n",
    "## Part 2: Detailed Exploration through Recursive Partitioning\n",
    "\n",
    "After initial assessments with the whole dataset, we delve deeper by dissecting the data into more homogenous segments. This step employs recursive partitioning, focusing on pivotal features to identify subsets that share common characteristics. Each subset is then modeled using Gaussian Mixture Models (GMM) to grasp the underlying distribution of events over time. This intricate process allows for a nuanced understanding of the data's structure and variability.\n",
    "\n",
    "### Implementation Steps:\n",
    "\n",
    "1. **Feature Identification using Decision Trees**: Utilize a Decision Tree Regressor to pinpoint the most influential feature that impacts the timing of events. This method ensures that the partitioning is guided by data-driven insights, leading to meaningful segmentation.\n",
    "\n",
    "2. **Recursive Partitioning**: Systematically divide the dataset based on the identified feature. This process is recursive, meaning it repeats this division within each resulting subset until a specified depth or minimum subset size is reached. This iterative approach ensures that even the most granular patterns can be uncovered.\n",
    "\n",
    "3. **Gaussian Mixture Modeling**: For each subset created through partitioning, a GMM is fitted. This step is crucial for understanding how events are distributed over time within each segment. By modeling these distributions, we can identify unique patterns that were not apparent in the aggregated data.\n",
    "\n",
    "4. **Visual Analysis**: Select subsets are visualized to showcase their GMM fits, providing a graphical representation of the time-based event distribution and the effectiveness of the segmentation.\n",
    "\n",
    "5. **Averaging Metrics**: Aggregate measures, including the average BIC score and the average number of components across all subsets, are computed. These metrics offer insights into the overall complexity and homogeneity of the data after segmentation. Lower BIC scores and fewer components suggest that the recursive partitioning has successfully identified more uniform groups within the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45f68e878787a47c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "\n",
    "# Function to identify the most important feature\n",
    "def plot_subset_gmm_with_components(current_data, subset_index):\n",
    "    subset_conditions, subset_data = final_partitions[subset_index]\n",
    "    conditions_str = ', '.join(subset_conditions) if subset_conditions else \"No conditions (initial dataset)\"\n",
    "    print(f\"Conditions for subset {subset_index}: {conditions_str}\")\n",
    "\n",
    "    # Prepare the data for GMM fitting\n",
    "    X = subset_data['time_of_day'].values.reshape(-1, 1)\n",
    "\n",
    "    if len(X) > 1:  # Ensure there are enough samples for GMM fitting\n",
    "        n_components_range = range(1, min(len(X), 11))\n",
    "        best_gmm = None\n",
    "        lowest_bic = np.infty\n",
    "        for n_components in n_components_range:\n",
    "            gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=0).fit(X)\n",
    "            bic = gmm.bic(X)\n",
    "            if bic < lowest_bic:\n",
    "                lowest_bic = bic\n",
    "                best_gmm = gmm\n",
    "\n",
    "        # Plotting the overall GMM fit\n",
    "        plt.hist(X.flatten(), bins=24, density=True, alpha=0.6, color='skyblue', label='Data Histogram')\n",
    "        x = np.linspace(X.min(), X.max(), 1000).reshape(-1, 1)\n",
    "        logprob = best_gmm.score_samples(x)\n",
    "        pdf = np.exp(logprob)\n",
    "        plt.plot(x, pdf, '-k', label='Overall GMM Fit')\n",
    "\n",
    "        # Plotting each component\n",
    "        for i in range(best_gmm.n_components):\n",
    "            mean = best_gmm.means_[i][0]\n",
    "            cov = best_gmm.covariances_[i][0][0]\n",
    "            component_pdf = np.exp(-0.5 * (x - mean) ** 2 / cov) / np.sqrt(2 * np.pi * cov) * best_gmm.weights_[i]\n",
    "            plt.plot(x, component_pdf, '--', label=f'Component {i+1}')\n",
    "\n",
    "        plt.title(f'Subset {subset_index}: Time of Day and GMM Density Estimation')\n",
    "        plt.xlabel('Time of Day')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Additional Information\n",
    "        # Histogram Data\n",
    "        hist_data, bin_edges = np.histogram(X, bins=24, density=True)\n",
    "        bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "        # GMM Parameters\n",
    "        gmm_means = best_gmm.means_.flatten()\n",
    "        gmm_variances = best_gmm.covariances_.flatten()\n",
    "        gmm_weights = best_gmm.weights_.flatten()\n",
    "\n",
    "        print(\"Histogram Bin Centers:\", bin_centers)\n",
    "        print(\"Histogram Bin Heights:\", hist_data)\n",
    "        print(\"Bin Edges:\", bin_edges)\n",
    "        print(\"GMM Means:\", gmm_means)\n",
    "        print(\"GMM Variances:\", gmm_variances)\n",
    "        print(\"GMM Weights:\", gmm_weights)\n",
    "\n",
    "    else:\n",
    "        print(\"Not enough data for GMM fitting.\")\n",
    "        \n",
    "def train_decision_tree_and_identify_important_feature_optimized(data, target_column_name):\n",
    "    data_copy = data.copy()\n",
    "    if not np.issubdtype(data_copy[target_column_name].dtype, np.number):\n",
    "        data_copy[target_column_name] = pd.to_numeric(data_copy[target_column_name], errors='coerce')\n",
    "    data_copy = data_copy.dropna(subset=[target_column_name])\n",
    "\n",
    "    categorical_cols = data_copy.select_dtypes(include=['object', 'bool']).columns\n",
    "    le = LabelEncoder()\n",
    "    for column in categorical_cols:\n",
    "        data_copy[column] = le.fit_transform(data_copy[column].astype(str))\n",
    "\n",
    "    cols_to_drop = ['case:ApplicationID', 'EventID', 'time:timestamp']\n",
    "    data_copy = data_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    X = data_copy.drop(columns=[target_column_name])\n",
    "    y = data_copy[target_column_name].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "    tree_reg.fit(X_train, y_train)\n",
    "\n",
    "    feature_importances = tree_reg.feature_importances_\n",
    "    most_important_feature_index = np.argmax(feature_importances)\n",
    "    most_important_feature_name = X.columns[most_important_feature_index]\n",
    "\n",
    "    return most_important_feature_name\n",
    "\n",
    "def segment_data_based_on_feature(current_data, feature_name):\n",
    "    segmented_data = {}\n",
    "    feature_values = current_data[feature_name].values\n",
    "\n",
    "    if np.issubdtype(current_data[feature_name].dtype, np.number):\n",
    "        unique_values = np.unique(feature_values)\n",
    "        if len(unique_values) > 10:  # Adjust this threshold as needed\n",
    "            feature_values_reshaped = feature_values.reshape(-1, 1)\n",
    "            n_components_range = range(1, min(len(unique_values), 11))\n",
    "            models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(feature_values_reshaped) for n in n_components_range]\n",
    "            best_gmm = min(models, key=lambda model: model.bic(feature_values_reshaped))\n",
    "            clusters = best_gmm.predict(feature_values_reshaped)\n",
    "            current_data['cluster'] = clusters\n",
    "            for cluster in np.unique(clusters):\n",
    "                cluster_data = current_data[current_data['cluster'] == cluster]\n",
    "                segmented_data[f'Cluster {cluster}'] = cluster_data.drop(columns=['cluster'])\n",
    "        else:\n",
    "            for value in unique_values:\n",
    "                segmented_data[f'{feature_name}={value}'] = current_data[current_data[feature_name] == value]\n",
    "    else:\n",
    "        unique_values = current_data[feature_name].unique()\n",
    "        for value in unique_values:\n",
    "            segmented_data[f'{feature_name}={value}'] = current_data[current_data[feature_name] == value]\n",
    "\n",
    "    return segmented_data\n",
    "\n",
    "def recursive_partition(current_data, excluded_features, threshold, max_depth, current_depth=0, conditions=[]):\n",
    "    if current_depth >= max_depth or len(current_data) <= threshold:\n",
    "        return [(conditions, current_data)]\n",
    "\n",
    "    features = [col for col in current_data.columns if col not in excluded_features + ['time_of_day']]\n",
    "    data_for_tree = current_data[features + ['time_of_day']].dropna()\n",
    "    if data_for_tree.empty or len(features) == 0:\n",
    "        return [(conditions, current_data)]\n",
    "\n",
    "    important_feature = train_decision_tree_and_identify_important_feature_optimized(data_for_tree, 'time_of_day')\n",
    "    if important_feature not in excluded_features:\n",
    "        partitions = segment_data_based_on_feature(current_data, important_feature)\n",
    "        results = []\n",
    "        for segment_key, segment_data in partitions.items():\n",
    "            new_conditions = conditions + [segment_key]\n",
    "            results += recursive_partition(segment_data, excluded_features + [important_feature], threshold, max_depth, current_depth + 1, new_conditions)\n",
    "        return results\n",
    "    else:\n",
    "        return [(conditions, current_data)]\n",
    "\n",
    "# Function to calculate averages\n",
    "def calculate_averages(final_partitions):\n",
    "    total_components = 0\n",
    "    total_bic = 0\n",
    "    for _, subset_data in final_partitions:\n",
    "        X = subset_data['time_of_day'].values.reshape(-1, 1)\n",
    "        if len(X) > 1:\n",
    "            n_components_range = range(1, min(len(X), 11))\n",
    "            lowest_bic = np.infty\n",
    "            best_n_components = None\n",
    "            for n_components in n_components_range:\n",
    "                gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=0).fit(X)\n",
    "                bic = gmm.bic(X)\n",
    "                if bic < lowest_bic:\n",
    "                    lowest_bic = bic\n",
    "                    best_n_components = n_components\n",
    "            total_components += best_n_components\n",
    "            total_bic += lowest_bic\n",
    "    average_components = total_components / len(final_partitions)\n",
    "    average_bic = total_bic / len(final_partitions)\n",
    "    return average_components, average_bic\n",
    "\n",
    "\n",
    "excluded_features = ['case:ApplicationID', 'OfferID', 'EventID', 'EventOrigin', 'case:concept:name', 'cluster', 'time:timestamp']\n",
    "final_partitions = recursive_partition(data.copy(), excluded_features, 100, 4)\n",
    "average_components, average_bic_total_subsets = calculate_averages(final_partitions)\n",
    "print(final_partitions)\n",
    "print(f\"Average number of GMM components: {average_components}\")\n",
    "print(f\"Average BIC score: {average_bic_total_subsets}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83526ef7a15311e7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subsets that are modelled with one normal distribution component\n",
    "## Part 3: Single-Component Optimal Fit Filtering and Averages Calculation\n",
    "\n",
    "In this section, the aim is to refine our analysis by focusing on subsets where a Gaussian Mixture Model (GMM) with a single component is deemed optimal. This constraint allows for a more targeted examination of data segments that exhibit a relatively uniform distribution of event times throughout the day.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Filtering**: Exclude subsets that are best modeled with more than one GMM component.\n",
    "2. **Average BIC Calculation**: Calculate the average Bayesian Information Criterion (BIC) score across the filtered subsets to evaluate the fit's quality.\n",
    "3. **Interpretation**: The resulting average provides insights into how well a simple model can represent the data, offering a perspective on the underlying process complexity.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27cb1c5c3d20962e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Function to filter subsets optimally modeled by a single GMM component and calculate their BIC scores\n",
    "def filter_and_calculate_bic_single_component_subsets(final_partitions):\n",
    "    bic_scores = []\n",
    "    single_component_subsets = []\n",
    "\n",
    "    for partition in final_partitions:\n",
    "        _, subset_data = partition\n",
    "        X = subset_data['time_of_day'].values.reshape(-1, 1)\n",
    "        if len(X) > 1:\n",
    "            best_bic = np.inf\n",
    "            best_model = None\n",
    "            for n_components in range(1, min(11, len(X) + 1)):\n",
    "                gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=0).fit(X)\n",
    "                bic = gmm.bic(X)\n",
    "                if bic < best_bic:\n",
    "                    best_bic = bic\n",
    "                    best_model = gmm\n",
    "            # Check if the best model for this subset has exactly one component\n",
    "            if best_model.n_components == 1:\n",
    "                bic_scores.append(best_bic)\n",
    "                single_component_subsets.append((best_model, X))\n",
    "\n",
    "    return bic_scores, single_component_subsets\n",
    "\n",
    "# Assuming final_partitions is already defined from the segmentation step\n",
    "bic_scores, single_component_subsets = filter_and_calculate_bic_single_component_subsets(final_partitions)\n",
    "\n",
    "# Calculate and print the average BIC score across subsets optimally modeled with a single component\n",
    "if bic_scores:\n",
    "    average_bic_single_component = np.mean(bic_scores)\n",
    "    print(f\"Average BIC score for subsets best modeled with a single GMM component: {average_bic_single_component:.2f}\")\n",
    "    print(f\"Number of subsets with a single component optimal: {len(bic_scores)}\")\n",
    "else:\n",
    "    print(\"No subsets are best modeled with a single GMM component.\")\n",
    "\n",
    "# Plotting one of the subsets randomly if available\n",
    "if single_component_subsets:\n",
    "    best_model, X = single_component_subsets[7]\n",
    "    plt.hist(X.flatten(), bins=24, density=True, alpha=0.6, color='skyblue', label='Data Histogram')\n",
    "    x = np.linspace(X.min(), X.max(), 1000).reshape(-1, 1)\n",
    "    logprob = best_model.score_samples(x)\n",
    "    pdf = np.exp(logprob)\n",
    "    plt.plot(x, pdf, '-k', label='Optimal Single Component GMM Fit')\n",
    "\n",
    "    # Since it's a single component, extract its mean and covariance for plotting\n",
    "    mean = best_model.means_[0]\n",
    "    covariance = best_model.covariances_[0]\n",
    "    component_pdf = np.exp(-0.5 * (x - mean) ** 2 / covariance) / np.sqrt(2 * np.pi * covariance)\n",
    "    plt.plot(x, component_pdf, '--r', label='GMM Component')\n",
    "\n",
    "    plt.title(\"Random Subset with Optimal Single Component GMM\")\n",
    "    plt.xlabel(\"Time of Day\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Additional Information\n",
    "    # Histogram Data\n",
    "    hist_data, bin_edges = np.histogram(X, bins=24, density=True)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "    # GMM Parameters\n",
    "    gmm_means = best_model.means_.flatten()\n",
    "    gmm_variances = best_model.covariances_.flatten()\n",
    "    gmm_weights = best_model.weights_.flatten()\n",
    "\n",
    "    print(\"Histogram Bin Centers:\", bin_centers)\n",
    "    print(\"Histogram Bin Heights:\", hist_data)\n",
    "    print(\"Bin Edges:\", bin_edges)\n",
    "    print(\"GMM Means:\", gmm_means)\n",
    "    print(\"GMM Variances:\", gmm_variances)\n",
    "    print(\"GMM Weights:\", gmm_weights)\n",
    "else:\n",
    "    print(\"No suitable subsets for plotting.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fee4d3a5473eff01",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing the Models\n",
    "## Part 4: Comparison of BIC Scores Across Different Models\n",
    "In this part, we compare the Bayesian Information Criterion (BIC) scores obtained from three different modeling approaches to understand their effectiveness in capturing the underlying distribution of the data:\n",
    "\n",
    "1. **Overall Data Model**: The BIC score for the entire dataset modeled with a Gaussian Mixture Model (GMM) to find the best fit.\n",
    "2. **Segmented Subsets Model**: After segmenting the data based on significant features, we model each subset with GMMs and calculate the average BIC score across all subsets.\n",
    "3. **Single-Component Subsets Model**: Focusing on subsets that are optimally modeled with a single component GMM, we calculate the average BIC score for these subsets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7d477c67c81fbff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Labels for each category\n",
    "categories = ['Overall Data', 'Segmented Subsets', 'Single-Component-Optimal Subsets']\n",
    "categories_comparison = ['Segmented Subsets', 'Single-Component-Optimal Subsets']\n",
    "\n",
    "# BIC scores for each category\n",
    "bic_scores = [lowest_bic, average_bic_total_subsets, average_bic_single_component]\n",
    "bic_scores_comparison = [average_bic_total_subsets, average_bic_single_component]\n",
    "\n",
    "# Creating the bar chart with a logarithmic scale\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, bic_scores, color=['blue', 'green', 'red'])\n",
    "plt.yscale('log')  # Applying logarithmic scale\n",
    "\n",
    "# Adding the title and labels\n",
    "plt.title('Comparison of BIC Scores Across Different Models (Log Scale)')\n",
    "plt.ylabel('BIC Score (log scale)')\n",
    "plt.xlabel('Model Type')\n",
    "\n",
    "# Showing the plot with log scale\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Creating a separate bar chart with a normal scale for segmented vs single-component\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories_comparison, bic_scores_comparison, color=['green', 'red'])\n",
    "\n",
    "# Adding the title and labels for the comparison\n",
    "plt.title('Comparison of BIC Scores: Segmented vs Single-Component')\n",
    "plt.ylabel('BIC Score')\n",
    "plt.xlabel('Model Type')\n",
    "\n",
    "# Showing the plot with normal scale\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of subsets\n",
    "total_subsets_count = len(final_partitions)  # Assuming 'final_partitions' contains all subsets\n",
    "single_component_optimal_count = len(single_component_subsets)  # Assuming 'single_component_subsets' contains only those optimal for a single component\n",
    "\n",
    "# Calculating fractions\n",
    "fractions = [single_component_optimal_count, total_subsets_count - single_component_optimal_count]\n",
    "labels = ['Single-Component-Optimal Subsets', 'Other Subsets']\n",
    "\n",
    "# Creating the pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(fractions, labels=labels, autopct='%1.1f%%', startangle=140, colors=['red', 'grey'])\n",
    "\n",
    "plt.title('Fraction of Single-Component-Optimal Subsets')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a52e0709b2aa564",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "## Optimizing Recursive Partitioning and GMM Fitting\n",
    "\n",
    "To fine-tune the segmentation and modeling of our dataset, we employ a methodical approach to identify the optimal parameters for recursive partitioning and Gaussian Mixture Model (GMM) fitting. Specifically, we aim to determine the best values for the **threshold** for partitioning and the **maximum recursion depth**. These parameters significantly influence the granularity of segmentation and, consequently, the fitting of GMMs to these segments.\n",
    "\n",
    "### Implementation Steps:\n",
    "\n",
    "1. **Parameter Variation**: We iterate over a predefined range of values for both the threshold and max_depth parameters. The threshold controls the minimum size a segment must have to be considered for further partitioning, while max_depth limits the recursion level to prevent over-segmentation.\n",
    "   \n",
    "2. **Evaluation with Average BIC**: For each parameter configuration, we use the recursive partitioning function to segment the dataset accordingly. Then, we fit GMMs to each segment and calculate the Bayesian Information Criterion (BIC) score for these models. The BIC score helps us evaluate the model fit while penalizing over-complexity. We calculate the average BIC score across all segments for each parameter setup.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82d192e9bd9a09cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming data is prepared and final_partitions is generated correctly\n",
    "\n",
    "# Define ranges of parameters to test\n",
    "thresholds = [10, 50, 100, 500, 1000]  # Example threshold values\n",
    "depths = [1, 2, 4, 8]  # Example max_depth values\n",
    "\n",
    "# Prepare to store results\n",
    "results = []\n",
    "\n",
    "# Nested loop over both thresholds and depths\n",
    "for threshold in thresholds:\n",
    "    for depth in depths:\n",
    "        final_partitions = recursive_partition(data.copy(), excluded_features, threshold, depth)\n",
    "        _, average_bic = calculate_averages(final_partitions)\n",
    "        results.append((threshold, depth, average_bic))\n",
    "\n",
    "# Extracting results for plotting\n",
    "thresholds, depths, bics = zip(*results)\n",
    "print(bics)\n",
    "# Normalize BIC values for point sizes, ensure division by zero is handled\n",
    "bic_min = min(bics)\n",
    "bic_max = max(bics)\n",
    "sizes = [20 + 180 * (bic - bic_min) / (bic_max - bic_min) if bic_max - bic_min > 0 else 100 for bic in bics]\n",
    "\n",
    "# Find optimal combination (minimal BIC)\n",
    "optimal_index = np.argmin(bics)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for all combinations\n",
    "ax.scatter(thresholds, depths, bics, s=sizes, c='blue', alpha=0.6)\n",
    "\n",
    "# Highlight the optimal combination\n",
    "ax.scatter([thresholds[optimal_index]], [depths[optimal_index]], [bics[optimal_index]], s=[sizes[optimal_index]], c='red', alpha=1, edgecolor='black', label='Optimal BIC')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Max Depth')\n",
    "ax.set_zlabel('Average BIC')\n",
    "ax.set_title('Average BIC vs Threshold vs Max Depth')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "605b107112ded242",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 5: Hyperparameter Tuning and Complexity Analysis\n",
    "\n",
    "In this part of the analysis, we embark on a meticulous journey to refine our partitioning strategy by tuning the hyperparameters and evaluating the complexity of our recursive partitioning algorithm. This process is crucial for optimizing the performance of our model and ensuring that we strike a perfect balance between model complexity and interpretability.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "To fine-tune the parameters of our recursive partitioning process, we explore a range of values for two critical hyperparameters: the `threshold` for the minimum subset size and the `max_depth` of recursive partitioning. The objective is to identify the optimal combination of these parameters that minimizes the Bayesian Information Criterion (BIC) score, indicating a better model fit with lower complexity.\n",
    "\n",
    "### Implementation Steps:\n",
    "\n",
    "1. We conduct a comprehensive grid search over specified ranges of `threshold` and `max_depth` values.\n",
    "2. For each combination of parameters, we execute the recursive partitioning function to segment the dataset and then fit Gaussian Mixture Models (GMM) to each subset.\n",
    "3. We calculate the average BIC score across all partitions for each parameter combination, aiming to find the set of parameters that yield the lowest average BIC.\n",
    "\n",
    "### Results\n",
    "\n",
    "A 3D scatter plot visualizes the outcomes of this hyperparameter tuning process, with each point representing a combination of `threshold` and `max_depth` values. The size of each point correlates with the BIC score, where larger points signify higher BIC values. The optimal combination, denoted by a distinct color, highlights the parameters that achieve the lowest average BIC, signifying the most efficient partitioning strategy.\n",
    "\n",
    "## Complexity Analysis\n",
    "\n",
    "Following the hyperparameter tuning, we delve into the complexity analysis of our recursive partitioning approach. This analysis aims to understand how the execution time of our partitioning algorithm varies with different `threshold` and `max_depth` values, providing insights into the scalability and efficiency of our method.\n",
    "\n",
    "### Implementation Steps:\n",
    "\n",
    "1. We measure the execution time for varying `threshold` values while keeping the `max_depth` constant and vice versa.\n",
    "2. This procedure allows us to observe the impact of each hyperparameter on the algorithm's computational demands.\n",
    "\n",
    "### Results\n",
    "\n",
    "Two line plots showcase the execution times against varying `threshold` and `max_depth` values. These plots reveal trends in how the complexity of our partitioning approach scales with the size of the subsets and the depth of recursion. A thorough understanding of these dynamics assists in making informed decisions when adjusting hyperparameters to balance model performance and computational efficiency.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3c72ed59d5ddeb2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Assuming 'data' is your DataFrame prepared for the analysis\n",
    "\n",
    "# Define the range of values for threshold and max depth\n",
    "threshold_values = np.arange(1000, 10001, 2000)  # Example: from 100 to 1000 in steps of 100\n",
    "max_depth_values = np.arange(1, 6, 1)  # Example: from 1 to 10 in steps of 1\n",
    "\n",
    "# Lists to store execution times\n",
    "times_threshold = []\n",
    "times_depth = []\n",
    "\n",
    "# Measure time for varying thresholds\n",
    "for threshold in threshold_values:\n",
    "    start_time = time.time()\n",
    "    final_partitions = recursive_partition(data.copy(), excluded_features, threshold, 5)  # Use a default depth value that makes sense for your dataset\n",
    "    calculate_averages(final_partitions)\n",
    "    end_time = time.time()\n",
    "    times_threshold.append(end_time - start_time)\n",
    "\n",
    "# Measure time for varying max depths\n",
    "for max_depth in max_depth_values:\n",
    "    start_time = time.time()\n",
    "    final_partitions = recursive_partition(data.copy(), excluded_features, 1000, max_depth)  # Use a default threshold that makes sense for your dataset\n",
    "    calculate_averages(final_partitions)\n",
    "    end_time = time.time()\n",
    "    times_depth.append(end_time - start_time)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(threshold_values, times_threshold, marker='o', color='blue')\n",
    "plt.title('Execution Time vs. Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(max_depth_values, times_depth, marker='o', color='green')\n",
    "plt.title('Execution Time vs. Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6302e5a2fc3623b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(threshold_values)\n",
    "print(times_threshold)\n",
    "print(max_depth_values)\n",
    "print(times_depth)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "469a8d8f46db18a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(bics)\n",
    "print(depths)\n",
    "print(thresholds)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bba30880811469ce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(categories)\n",
    "print(bic_scores)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c0c2726a05dd162",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(best_gmm)\n",
    "print(best_gmm.means_)\n",
    "print(best_gmm.covariances_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aed8a5952d9ec634",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
